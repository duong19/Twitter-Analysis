version: "3.6"



services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    deploy:
      replicas: 1
    ports:
      - 9870:9870
     
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    # container_name: datanode
    restart: always
    deploy:
      replicas: 1
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    expose:
      - "2181"
    ports:
      - 2181:2181
           
  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    expose:
      - "9092"
    ports:
      - 9092:9092
      - 19092:19092
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://localhost:19092
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://0.0.0.0:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      JMX_PORT: 9999
  kafka_manager:
    image: hlebalbau/kafka-manager:1.3.3.18
    container_name: kafka_manager
    expose:
      - "9000"
    ports:
      - 9000:9000
    environment:
      ZK_HOSTS: "zookeeper:2181"
      APPLICATION_SECRET: "random-secret"
      command: -Dpidfile.path=/dev/null
    links:
        - kafka
        - zookeeper
    depends_on:
        - "zookeeper"
        - "kafka"

  # spark-master:
  #   build: ./spark/master
  #   container_name: spark-master
  #   ports:
  #     - "8080:8080"
  #     - "7077:7077"
  #   environment:
  #     - INIT_DAEMON_STEP=setup_spark
  #     - PYSPARK_PYTHON=/usr/bin/python3
  #     - PYSPARK_DRIVER_PYTHON=/usr/bin/python3

  # spark-worker-1:
  #   build: ./spark/worker/
  #   container_name: spark-worker-1
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     - "SPARK_MASTER=spark://spark-master:7077"
  #     - SPARK_WORKER_CORES=2
  #     - SPARK_WORKER_MEMORY=512m
  #     - PYSPARK_PYTHON=/usr/bin/python3
  #     - PYSPARK_DRIVER_PYTHON=/usr/bin/python3

  # spark-worker-2:
  #   image: bde2020/spark-worker:3.0.1-hadoop3.2
  #   container_name: spark-worker-2
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "8081:8081"
  #   environment:
  #     - SPARK_MASTER=spark://spark-master:7077
  #     - SPARK_WORKER_CORES=2
  #     - SPARK_WORKER_MEMORY=512m
  #     - PYSPARK_PYTHON=/usr/bin/python3
  #     - PYSPARK_DRIVER_PYTHON=/usr/bin/python3

  pyspark:
    build: ./pyspark/
    container_name: pyspark-notebook
    ports:
      - "8888:8888"
    volumes:
      - ./pyspark/:/home/jovyan/work/
    healthcheck:
      interval: 5s
      retries: 100 
    env_file:
      - ./hadoop.env


      
  nodejs:
    build: ./streaming
    container_name: node-js
    
    volumes:
      - ./streaming:/usr/app/streaming
      - ./streaming/node_modules:/usr/app/streaming/node_modules
    # environment: 
    #   TZ: 
    command: npm run dev
    # networks: 
    #   - app-network
    depends_on: 
      - 'kafka'

  spark-master:
    build: ./spark/master
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    volumes:
      - shared-workspace:/opt/workspace
  spark-worker-1:
    build: ./spark/worker
    container_name: spark-worker-1
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1g
    ports:
      - 8081:8081
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
  spark-worker-2:
    build: ./spark/worker
    container_name: spark-worker-2
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=1g
    ports:
      - 8082:8081
    volumes:
      - shared-workspace:/opt/workspace
    depends_on:
      - spark-master
    


volumes:
  hadoop_namenode:
  hadoop_datanode:
  shared-workspace:
    name: "hadoop-distributed-file-system"
    driver: local